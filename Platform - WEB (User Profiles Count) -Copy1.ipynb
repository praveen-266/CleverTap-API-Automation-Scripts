{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5890f1f2-bc29-4896-8426-7d726e3d89b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Date     Metric Name channel_name  count\n",
      "0  2024-08-27  Platform - WEB  Middle East    448\n",
      "1  2024-08-27  Platform - WEB        India   4068\n",
      "2  2024-08-27  Platform - WEB    Singapore    276\n",
      "3  2024-08-27  Platform - WEB     Malaysia    158\n",
      "4  2024-08-27  Platform - WEB  Philippines    188\n",
      "5  2024-08-27  Platform - WEB    Indonesia      4\n",
      "6  2024-08-27  Platform - WEB     Thailand     10\n",
      "7  2024-08-27  Platform - WEB     Hongkong      1\n",
      "8  2024-08-27  Platform - WEB      Vietnam      4\n",
      "Execution time for PageViewed on 2024-08-27 16:27:48.684817: 120.75 seconds\n",
      "         Date     Metric Name channel_name  count\n",
      "0  2024-08-28  Platform - WEB        India   3695\n",
      "1  2024-08-28  Platform - WEB     Malaysia    156\n",
      "2  2024-08-28  Platform - WEB  Middle East    435\n",
      "3  2024-08-28  Platform - WEB    Singapore    310\n",
      "4  2024-08-28  Platform - WEB  Philippines    213\n",
      "5  2024-08-28  Platform - WEB     Hongkong      8\n",
      "6  2024-08-28  Platform - WEB    Indonesia      6\n",
      "7  2024-08-28  Platform - WEB     Thailand      6\n",
      "8  2024-08-28  Platform - WEB      Vietnam      0\n",
      "Execution time for PageViewed on 2024-08-28 16:27:48.684817: 113.89 seconds\n",
      "         Date     Metric Name channel_name  count\n",
      "0  2024-08-29  Platform - WEB  Philippines    468\n",
      "1  2024-08-29  Platform - WEB        India   4315\n",
      "2  2024-08-29  Platform - WEB    Singapore    454\n",
      "3  2024-08-29  Platform - WEB  Middle East    571\n",
      "4  2024-08-29  Platform - WEB     Malaysia    245\n",
      "5  2024-08-29  Platform - WEB     Thailand     10\n",
      "6  2024-08-29  Platform - WEB    Indonesia      1\n",
      "7  2024-08-29  Platform - WEB      Vietnam      3\n",
      "8  2024-08-29  Platform - WEB     Hongkong      2\n",
      "Execution time for PageViewed on 2024-08-29 16:27:48.684817: 105.31 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "MAX_RETRIES = 10\n",
    "RETRY_DELAY = 90  # Delay between retries\n",
    "\n",
    "# Get today's date and calculate the start date (1 day before today)\n",
    "end_date = datetime.now()\n",
    "start_date = end_date - timedelta(days=3)\n",
    "\n",
    "# Define the list of events\n",
    "events = [\"PageViewed\"]\n",
    "\n",
    "output_file = r'C:\\Users\\praveen\\Platform(Web).xlsx'\n",
    "\n",
    "\n",
    "# Define the mapping dictionary\n",
    "mapping = {\n",
    "    'india': 'India',\n",
    "    'philippines': 'Philippines',\n",
    "    'malaysia': 'Malaysia',\n",
    "    'singapore': 'Singapore',\n",
    "    'gulf': 'Middle East',\n",
    "    'hongkong': 'Hongkong',\n",
    "    'thailand': 'Thailand',\n",
    "    'indonesia': 'Indonesia',\n",
    "    'vietnam': 'Vietnam',\n",
    "    'rexmonster': 'India',\n",
    "}\n",
    "\n",
    "# Define the required columns\n",
    "required_columns = [\n",
    "    'India',\n",
    "    'Philippines',\n",
    "    'Malaysia',\n",
    "    'Singapore',\n",
    "    'Middle East',\n",
    "    'Hongkong',\n",
    "    'Thailand',\n",
    "    'Indonesia',\n",
    "    'Vietnam',\n",
    "]\n",
    "\n",
    "# Function to validate and parse date format\n",
    "def validate_date_format(date_str):\n",
    "    try:\n",
    "        return datetime.strptime(date_str, '%d/%m/%Y')\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "\n",
    "# Function to get event data for an event\n",
    "def get_event_data(event_name, date_integer):\n",
    "    url = \"https://in1.api.clevertap.com/1/events.json?batch_size=5000\"\n",
    "    payload = json.dumps({\n",
    "        \"event_name\": event_name,\n",
    "        \"from\": date_integer,\n",
    "        \"to\": date_integer,\n",
    "        \"common_profile_properties\": {\n",
    "            \"event_properties\": [\n",
    "                {\n",
    "                    \"name\": \"platform\",\n",
    "                    \"operator\": \"contains\",\n",
    "                    \"value\": \"Desktop\"\n",
    "                }\n",
    "            ],\n",
    "            \"reachability\": [{\n",
    "                \"name\": \"has_email\",\n",
    "                \"value\": \"True\"\n",
    "            }],\n",
    "            \"user_properties\": [\n",
    "                {\"name\": \"profile_last_updated\", \"operator\": \"equals\", \"value\": [date_integer]},\n",
    "                {\"name\": \"channel_name\", \"operator\": \"contains\", \"value\": list(mapping.keys())}\n",
    "            ]\n",
    "        }\n",
    "    })\n",
    "\n",
    "    headers = {\n",
    "        'X-CleverTap-Account-Id': '*******',\n",
    "        'X-CleverTap-Passcode': '********',\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "\n",
    "    retry_count = 0\n",
    "    count_country = {}\n",
    "    unique_users = set()  # Set to track unique users\n",
    "\n",
    "    while retry_count < MAX_RETRIES:\n",
    "        response = requests.post(url, headers=headers, data=payload)\n",
    "        response_json = response.json()\n",
    "\n",
    "        if response_json.get('status') == 'fail' and response_json.get('code') == 2:\n",
    "            retry_count += 1\n",
    "            print(f\"Request still in progress, please retry later. Retrying... Attempt {retry_count}\")\n",
    "            time.sleep(RETRY_DELAY * retry_count)\n",
    "            continue\n",
    "        elif response_json.get('status') == 'fail':\n",
    "            print(f\"Failed request with error: {response_json.get('error')}\")\n",
    "            return {}\n",
    "\n",
    "        cursor = response_json.get('cursor')\n",
    "\n",
    "        while cursor:\n",
    "            next_url = f\"https://in1.api.clevertap.com/1/events.json?cursor={cursor}\"\n",
    "            response = requests.get(next_url, headers=headers)\n",
    "\n",
    "            try:\n",
    "                response_json = response.json()\n",
    "                cursor = response_json.get('next_cursor')\n",
    "\n",
    "                if 'records' in response_json:\n",
    "                    for record in response_json['records']:\n",
    "                        profile = record.get('profile', {})\n",
    "                        user_id = profile.get('identity')\n",
    "\n",
    "                        # Validate platform and profile last updated\n",
    "                        profile_data = profile.get('profileData', {})\n",
    "                        profile_last_updated = profile_data.get('profile_last_updated', '')\n",
    "\n",
    "                        # Check if platform, profile last updated match, and email is present\n",
    "                        if (record.get('event_props', {}).get('platform') == 'Desktop' and\n",
    "                            validate_date_format(profile_last_updated) and\n",
    "                            validate_date_format(profile_last_updated).date() == datetime.strptime(str(date_integer), '%Y%m%d').date()):\n",
    "\n",
    "                            if user_id and user_id not in unique_users:\n",
    "                                unique_users.add(user_id)\n",
    "\n",
    "                                # Retrieve channel_name\n",
    "                                channel_name = profile_data.get('channel_name', '')\n",
    "\n",
    "                                # Handle multiple channel names and case insensitivity\n",
    "                                if isinstance(channel_name, list):\n",
    "                                    channel_name = ' '.join(channel_name)\n",
    "                                channel_name = channel_name.lower().strip()\n",
    "\n",
    "                                # Check if channel_name is in the mapping or a substring of a mapped value\n",
    "                                for mapped_key, mapped_value in mapping.items():\n",
    "                                    if mapped_key in channel_name or channel_name in mapped_value:\n",
    "                                        if mapped_value not in count_country:\n",
    "                                            count_country[mapped_value] = 1\n",
    "                                        else:\n",
    "                                            count_country[mapped_value] += 1\n",
    "                                        break\n",
    "\n",
    "                if not cursor:\n",
    "                    break\n",
    "\n",
    "            except ValueError:\n",
    "                print(f\"Error decoding JSON: {response.text}\")\n",
    "                break\n",
    "\n",
    "        if count_country:\n",
    "            return count_country\n",
    "        else:\n",
    "            retry_count += 1\n",
    "            print(f\"Retrying... Attempt {retry_count}\")\n",
    "            time.sleep(RETRY_DELAY * retry_count)\n",
    "\n",
    "    print(\"Max retries reached. Unable to fetch data.\")\n",
    "    return {}\n",
    "\n",
    "# Main logic to iterate over dates and fetch data\n",
    "current_date = start_date\n",
    "while current_date < end_date:\n",
    "    date_integer = int(current_date.strftime('%Y%m%d'))\n",
    "    for event in events:\n",
    "        start_time = time.time()  # Start timer\n",
    "        event_data = get_event_data(event, date_integer)\n",
    "        if not event_data:\n",
    "            print(f\"No data for event {event} on {current_date}\")\n",
    "            current_date += timedelta(days=1)\n",
    "            continue\n",
    "\n",
    "        df = pd.DataFrame(list(event_data.items()), columns=['channel_name', 'count'])\n",
    "        df['Metric Name'] = 'Platform - WEB' \n",
    "        df['Date'] = pd.to_datetime(current_date).date()\n",
    "\n",
    "        # Ensure all required columns are present\n",
    "        for column in required_columns:\n",
    "            if column not in df['channel_name'].values:\n",
    "                df = pd.concat([df, pd.DataFrame({'channel_name': [column], 'count': [0],\n",
    "                                                 'Metric Name': 'Platform - WEB',\n",
    "                                                 'Date': pd.to_datetime(current_date).date()})])\n",
    "\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "        df = df[['Date', 'Metric Name', 'channel_name', 'count']]\n",
    "\n",
    "        end_time = time.time()  # End timer\n",
    "\n",
    "        # Calculate execution time\n",
    "        execution_time = end_time - start_time  \n",
    "\n",
    "        print(df)\n",
    "        print(f\"Execution time for {event} on {current_date}: {execution_time:.2f} seconds\")\n",
    "\n",
    "        # Check if the file and sheet already exist\n",
    "        try:\n",
    "            with pd.ExcelWriter(output_file, mode='a', engine='openpyxl', if_sheet_exists='overlay') as writer:\n",
    "                book = writer.book\n",
    "                if 'Platform_web' in book.sheetnames:\n",
    "                    existing_data = pd.read_excel(output_file, sheet_name='Platform_web')\n",
    "                    if 'Date' not in existing_data.columns:\n",
    "                        raise KeyError(\"'Date' column is missing in the existing data\")\n",
    "\n",
    "                    # Ensure uniqueness in existing_data\n",
    "                    existing_data = existing_data.drop_duplicates(subset=['Date', 'channel_name'])\n",
    "                    df = df.drop_duplicates(subset=['Date', 'channel_name'])\n",
    "                    updated_data = pd.concat([existing_data, df], ignore_index=True)\n",
    "\n",
    "                    updated_data.to_excel(writer, sheet_name='Platform_web', index=False)\n",
    "                else:\n",
    "                    df.to_excel(writer, sheet_name='Platform_web', index=False)\n",
    "        except FileNotFoundError:\n",
    "            with pd.ExcelWriter(output_file, mode='w', engine='openpyxl') as writer:\n",
    "                df.to_excel(writer, sheet_name='Platform_web', index=False)\n",
    "\n",
    "    current_date += timedelta(days=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065dd311-ad70-4897-8b02-c475ed9f06d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
